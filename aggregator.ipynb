{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g65JK44dUAlc",
        "outputId": "48f71b03-dc3a-4514-dfe5-e9ef9257c593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import re\n",
        "import json\n",
        "from urllib.request import urlopen\n",
        "import requests\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import shutil\n",
        "from requests.exceptions import ChunkedEncodingError, RequestException, HTTPError\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/My Drive/Rallies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP3GAbX2sXZM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wILjQlxhHvde"
      },
      "outputs": [],
      "source": [
        "def get_ids_custom(url, items=[], params={\"fo\": \"json\", \"c\": 100, \"at\": \"results,pagination\"}):\n",
        "    \"\"\"\n",
        "    Retrieves all item IDs from the LOC API, handling pagination.\n",
        "\n",
        "    Args:\n",
        "        url (str): The base URL for the API request.\n",
        "        items (list, optional): A list to append fetched IDs to. Defaults to [].\n",
        "        params (dict, optional): API request parameters. Defaults to {\"fo\": \"json\", \"c\": 100, \"at\": \"results,pagination\"}.\n",
        "\n",
        "    Returns:\n",
        "        list: The list 'items' populated with all retrieved IDs.\n",
        "    \"\"\"\n",
        "    r = requests.get(url, params=params)\n",
        "    r.raise_for_status()\n",
        "    for result in r.json()['results']:\n",
        "        items.append(result.get('id'))\n",
        "    next_page = r.json()['pagination'].get('next')\n",
        "    count = 0\n",
        "    while next_page:\n",
        "        try:\n",
        "            r = requests.get(next_page, params=params)\n",
        "            r.raise_for_status()\n",
        "            for result in r.json()['results']:\n",
        "                items.append(result.get('id'))\n",
        "            next_page = r.json()['pagination'].get('next')\n",
        "            print(f\"Fetched {len(items)} items so far...\")\n",
        "            if count % 2 == 0:\n",
        "                print('Waiting for 5 seconds...')\n",
        "                time.sleep(5)\n",
        "            count += 1\n",
        "        except ChunkedEncodingError:\n",
        "            print(f\"ChunkedEncodingError encountered for {url}. Retrying after 15 seconds...\")\n",
        "            time.sleep(15)\n",
        "            try:\n",
        "                call_retry = requests.get(next_page, params=params)\n",
        "                call_retry.raise_for_status()\n",
        "                data = call_retry.json()\n",
        "                text = data.get('full_text')\n",
        "                items.append(text)\n",
        "                print(f\"Successfully downloaded result {next_page} on retry. Waiting 5 seconds...\")\n",
        "                time.sleep(5)\n",
        "            except (RequestException, HTTPError, ChunkedEncodingError) as retry_err:\n",
        "                print(f\"Retry failed for {next_page}. Skipping. Error: {retry_err}\")\n",
        "                items.append(np.nan)\n",
        "                continue\n",
        "            except Exception as retry_e:\n",
        "                print(f\"Retry failed during processing for {url}. Skipping. Error: {retry_e}\")\n",
        "                items.append(np.nan)\n",
        "                continue\n",
        "        except HTTPError as http_err:\n",
        "            if http_err.response.status_code == 429:\n",
        "                print(f'Too many requests (429) when accessing {url}. Stopping early.')\n",
        "                print(f'Current number of requests: {len(items)}.')\n",
        "                pagination = r.json()['pagination'].get('current')\n",
        "                print(f'Current pagination: {pagination}')\n",
        "                break\n",
        "            else:\n",
        "                print(f\"HTTP error for {next_page}: {http_err}. Skipping.\")\n",
        "                items.append(np.nan)\n",
        "                continue\n",
        "    return items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JK8lyzOasVob"
      },
      "outputs": [],
      "source": [
        "def get_full_text(results, checkpoint_path=\"loc_full_text.pkl\", sleep_time=4, checkpoint_interval=100, checkpoint_time_interval=600, candidate_name=None):\n",
        "    \"\"\"\n",
        "    Fetches full text and specified metadata from a list of LoC URLs, with automatic checkpointing.\n",
        "\n",
        "    Args:\n",
        "      results (list): List of URLs to fetch.\n",
        "      checkpoint_path (str): Path to checkpoint file.\n",
        "      sleep_time (int): Seconds to wait between successful requests.\n",
        "      checkpoint_interval (int): Save after this many downloads.\n",
        "      checkpoint_time_interval (int): Save after this many seconds have elapsed.\n",
        "      candidate_name (str, optional): Name of the candidate being processed.\n",
        "    Returns:\n",
        "      list: A list of lists, where each inner list contains [full_text, library_of_congress_control_number, location_city, location_state].\n",
        "            None or np.nan will be used for failures or missing data.\n",
        "    \"\"\"\n",
        "    # Ensure the 'pkl' directory exists\n",
        "    checkpoint_dir = os.path.join(os.path.dirname(checkpoint_path), 'pkl')\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    # Always save checkpoints in the 'pkl' directory\n",
        "    checkpoint_filename = os.path.basename(checkpoint_path)\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        with open(checkpoint_path, \"rb\") as f:\n",
        "            items = pickle.load(f)\n",
        "        start_idx = len(items)\n",
        "        print(f\"Resuming from checkpoint: {start_idx}/{len(results)} already done.\")\n",
        "    else:\n",
        "        items = []\n",
        "        start_idx = 0\n",
        "        print(\"No checkpoint found; starting from scratch.\")\n",
        "    last_save_time = time.time()\n",
        "    for i in range(start_idx, len(results)):\n",
        "        url = results[i]\n",
        "        print(f\"[{i+1}/{len(results)}] Downloading {url}\")\n",
        "        full_text = None\n",
        "        loc_control_number = None\n",
        "        location_city = None\n",
        "        location_state = None\n",
        "        date = None\n",
        "        try:\n",
        "            resp = requests.get(url, params={\"fo\": \"json\"})\n",
        "            resp.raise_for_status()\n",
        "            data = resp.json()\n",
        "            item_data = data.get(\"item\", {})\n",
        "            pagination_data = data.get(\"pagination\", {})\n",
        "            full_text = data.get(\"full_text\", None)\n",
        "            loc_control_number = item_data.get(\"library_of_congress_control_number\", None)\n",
        "            page_number = pagination_data.get(\"current\", None)\n",
        "            city_list = item_data.get(\"location_city\", None)\n",
        "            if city_list and isinstance(city_list, list):\n",
        "                location_city = city_list[0]\n",
        "            state_list = item_data.get(\"location_state\", None)\n",
        "            if state_list and isinstance(state_list, list):\n",
        "                location_state = state_list[0]\n",
        "            date = item_data.get(\"date\", None)\n",
        "            items.append([candidate_name if candidate_name else None, loc_control_number, date, location_city, location_state, page_number, full_text])\n",
        "        except ChunkedEncodingError:\n",
        "            print(\"  ChunkedEncodingError; retrying in 15sâ€¦\")\n",
        "            time.sleep(15)\n",
        "            try:\n",
        "                resp = requests.get(url, params={\"fo\": \"json\"})\n",
        "                resp.raise_for_status()\n",
        "                data = resp.json()\n",
        "                item_data = data.get(\"item\", {})\n",
        "                pagination_data = data.get(\"pagination\", {})\n",
        "                full_text = data.get(\"full_text\", None)\n",
        "                loc_control_number = item_data.get(\"library_of_congress_control_number\", None)\n",
        "                page_number = pagination_data.get(\"current\", None)\n",
        "                city_list = item_data.get(\"location_city\", [])\n",
        "                location_city = city_list[0] if city_list and isinstance(city_list, list) else None\n",
        "                state_list = item_data.get(\"location_state\", [])\n",
        "                location_state = state_list[0] if state_list and isinstance(state_list, list) else None\n",
        "                date = item_data.get(\"date\", None)\n",
        "                items.append([candidate_name if candidate_name else None, loc_control_number, date, location_city, location_state, page_number, full_text])\n",
        "            except Exception as e:\n",
        "                print(f\"  Retry failed: {e}. Appending np.nan for all fields.\")\n",
        "                items.append([np.nan, np.nan, np.nan, np.nan])\n",
        "        except HTTPError as http_err:\n",
        "            if http_err.response.status_code == 429:\n",
        "                print(\"  429 Too Many Requestsâ€”stopping early.\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"  HTTPError ({http_err.response.status_code}); skipping.\")\n",
        "                items.append([np.nan, np.nan, np.nan, np.nan])\n",
        "        except RequestException as req_err:\n",
        "            print(f\"  RequestException: {req_err}; skipping.\")\n",
        "            items.append([None, None, None, None])\n",
        "        except Exception as e:\n",
        "            print(f\"  Unexpected error: {e}; skipping.\")\n",
        "            items.append([np.nan, np.nan, np.nan, np.nan])\n",
        "        print(f\"  Done. Sleeping {sleep_time}sâ€¦\")\n",
        "        time.sleep(sleep_time)\n",
        "        now = time.time()\n",
        "        needs_save = (\n",
        "            (i + 1) % checkpoint_interval == 0\n",
        "            or (now - last_save_time) >= checkpoint_time_interval\n",
        "        )\n",
        "        if needs_save:\n",
        "            with open(checkpoint_path, \"wb\") as f:\n",
        "                pickle.dump(items, f)\n",
        "            print(f\"  â‡’ Checkpoint saved at index {i+1}.\")\n",
        "            last_save_time = now\n",
        "    with open(checkpoint_path, \"wb\") as f:\n",
        "        pickle.dump(items, f)\n",
        "    print(\"All done. Final checkpoint written.\")\n",
        "    return items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y10S4CuBH5im"
      },
      "outputs": [],
      "source": [
        "# This function is not used in the final code, but is kept for reference\n",
        "def to_link(row):\n",
        "    \"\"\"\n",
        "    Generates a Chronicling America URL based on a row from regex extracted link DataFrame.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row from prelink DataFrame with '0', '1', '2' as column names.\n",
        "\n",
        "    Returns:\n",
        "        str: The Chronicling America URL.\n",
        "    \"\"\"\n",
        "    base_url = 'http://www.loc.gov/resource/'\n",
        "    sn = row[2]\n",
        "    date = row[0]\n",
        "    page = row[1]\n",
        "    url = f\"{base_url}{sn}/{date}/ed-1/\"\n",
        "    if page is not None:\n",
        "        url += f\"?sp={page}\"\n",
        "    return url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08L5qqV4Ipq0"
      },
      "outputs": [],
      "source": [
        "def candidate_aggregator(file_name):\n",
        "    \"\"\"\n",
        "    Generates a list of Chronicling America URLs for searching candidate names from an Excel file.\n",
        "\n",
        "    Args:\n",
        "      file_name (str): The path to the Excel file containing 'Year' and\n",
        "                       'Candidate_var1' through 'Candidate_var4' columns.\n",
        "\n",
        "    Returns:\n",
        "      list: A list of strings, where each string is a fully constructed URL\n",
        "            for a Chronicling America search query.\n",
        "    \"\"\"\n",
        "    name_cols = [f'Candidate_var{i}' for i in range(1, 5)]\n",
        "    df = pd.read_excel(file_name, usecols=['Year'] + name_cols)\n",
        "    base_url = (\n",
        "        \"https://www.loc.gov/collections/chronicling-america/?dl=page\"\n",
        "        \"&start_date={year}-07-01&end_date={year}-11-15\"\n",
        "        \"&qs={terms}&ops={ops}&searchType=advanced&fo=json\"\n",
        "    )\n",
        "    queries = []\n",
        "    for row in df.itertuples(index=False):\n",
        "        year = int(row.Year)\n",
        "        names = [\n",
        "            str(getattr(row, col)).replace(\" \", \"+\")\n",
        "            for col in name_cols\n",
        "            if pd.notnull(getattr(row, col)) and not isinstance(getattr(row, col), int)\n",
        "        ]\n",
        "        terms = \"+\".join(f'\"{n}\"' for n in names)\n",
        "        ops = '\"\"' if len(names) == 1 else \"OR\"\n",
        "        queries.append(base_url.format(year=year, terms=terms, ops=ops))\n",
        "    return queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exo5HVU3QBlj"
      },
      "outputs": [],
      "source": [
        "def flatten_triple_nested_array(triple_nested_array):\n",
        "    \"\"\"\n",
        "    Flattens a triple-nested list into a single-nested list of document information.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        triple_nested_array (list): A list of lists, where the innermost lists\n",
        "                                     contain four elements: full text (str),\n",
        "                                     document ID (str), city (str), and state (str).\n",
        "\n",
        "    Returns:\n",
        "        list: A single-nested list where each sublist represents a document's\n",
        "              information (full text, id, city, state).\n",
        "    \"\"\"\n",
        "    flattened_data = []\n",
        "    for candidate_docs in triple_nested_array:\n",
        "        for doc_info in candidate_docs:\n",
        "            flattened_data.append(doc_info)\n",
        "    return flattened_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4BnCZxlbXDx-"
      },
      "outputs": [],
      "source": [
        "def flatten_triple_nested_array(triple_nested_array):\n",
        "    \"\"\"\n",
        "    Flattens a triple-nested list into a single-nested list of document information.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        triple_nested_array (list): A list of lists, where the innermost lists\n",
        "                                     contain four elements: full text (str),\n",
        "                                     document ID (str), city (str), and state (str).\n",
        "\n",
        "    Returns:\n",
        "        list: A single-nested list where each sublist represents a document's\n",
        "              information (full text, id, city, state).\n",
        "    \"\"\"\n",
        "    flattened_data = []\n",
        "    for candidate_docs in triple_nested_array:\n",
        "        for doc_info in candidate_docs:\n",
        "            flattened_data.append(doc_info)\n",
        "    return flattened_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jaAGShkmNAOM"
      },
      "outputs": [],
      "source": [
        "def complete_candidates_collector(xlsx, delete_pkl=False):\n",
        "    \"\"\"\n",
        "    Collects and processes Library of Congress data for presidential candidates.\n",
        "    This function orchestrates the entire data collection pipeline for presidential\n",
        "    candidates. It starts by generating search URLs from an Excel file, then\n",
        "    iterates through each candidate to collect document IDs and their full text\n",
        "    from the Library of Congress. Finally, it flattens the collected data and\n",
        "    saves it to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        xlsx (str): The path to the Excel file containing candidate information\n",
        "                    (e.g., 'RawData/AmericanStories/PresidentialCandidates_Wikipedia.xlsx').\n",
        "        delete_pkl (bool): Whether to delete the pkl directory after successful data collection.\n",
        "\n",
        "    Returns:\n",
        "        None: The function saves the processed data to a CSV file named\n",
        "                'LOC_Presidential_Candidates_Complete_Data.csv' and does not return\n",
        "                any value.\n",
        "    \"\"\"\n",
        "    print('Creating all links from inputted file...')\n",
        "    search_urls = candidate_aggregator(xlsx)\n",
        "    num_candidates = len(search_urls)\n",
        "    print(f'Done. Created {num_candidates} links.')\n",
        "    all_ids = []\n",
        "    for url in search_urls[:2]:\n",
        "        ids = []\n",
        "        filename = create_filename(url)\n",
        "        print(f'Starting ID Collection for Candidate {filename[0]}\\n')\n",
        "        get_ids_custom(url, items=ids)\n",
        "        # Save .pkl files in the 'pkl' directory\n",
        "        checkpoint_path = os.path.join('pkl', f'{filename[0]}.pkl')\n",
        "        texts = get_full_text(ids, checkpoint_path=checkpoint_path, candidate_name=filename[1])\n",
        "        all_ids.append(texts)\n",
        "        print(f'\\nFinished ID Collection for Candidate {filename[0]}')\n",
        "    print('All Data Collected. Concatenating all texts into DataFrame...')\n",
        "    flattened = flatten_triple_nested_array(all_ids)\n",
        "    df = pd.DataFrame(flattened, columns=['name', 'library_of_congress_control_number', 'date', 'location_city', 'location_state', 'page_number', 'full_text'])\n",
        "    df.to_csv('LOC_Presidential_Candidates_Complete_Data.csv')\n",
        "    print(\"Done. Saved all data to 'LOC_Presidential_Candidates_Complete_Data.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZcLJ_UHPPN3",
        "outputId": "ef4c3171-1f52-4934-b0b5-0a147d8a90bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating all links from inputted file...\n",
            "Done. Created 170 links.\n",
            "Starting ID Collection for Candidate George_Washington_1788-07-01_1788-11-15\n",
            "\n",
            "Resuming from checkpoint: 1/1 already done.\n",
            "All done. Final checkpoint written.\n",
            "\n",
            "Finished ID Collection for Candidate George_Washington_1788-07-01_1788-11-15\n",
            "Starting ID Collection for Candidate John_Adams_1788-07-01_1788-11-15\n",
            "\n",
            "Resuming from checkpoint: 11/11 already done.\n",
            "All done. Final checkpoint written.\n",
            "\n",
            "Finished ID Collection for Candidate John_Adams_1788-07-01_1788-11-15\n",
            "All Data Collected. Concatenating all texts into DataFrame...\n",
            "Done. Saved all data to 'LOC_Presidential_Candidates_Complete_Data.csv'\n"
          ]
        }
      ],
      "source": [
        "complete_candidates_collector('RawData/AmericanStories/PresidentialCandidates_Wikipedia.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8MSqyR5bQxW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NFi8v51waKP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dsc80",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
